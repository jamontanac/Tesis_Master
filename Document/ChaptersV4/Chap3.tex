\chapter{Exploiting The Tools From Theory Code in Fermionic Systems.}
\section{Mechanism Behind Typicality as a Random Minimum Code.}
As we mentioned in the first chapter, we are interested in studing the behaviour of states that are close in energy. First we will show why it is possible to study ultra-orthogonality for Fermionic states in two possibilities. The first one is in terms of minimum distance codes and we will see that ultra orthogonality turns out to be an exact result. For the second part, we deduce its correspondent exponent error. For the last part, we will tackle the problem when we can not work over a minimum distance and we will show how ultra orthogonality can be acting over this specific case. Even though for this case we were not able to generalised this to every Fermionic system, we will discuss why this result should holds in general.

\subsection{ultra orthogonality on fermions}
To start this part we want to recapitulate a couple of things: First, as we showed in equation \eqref{CH1:expansion_5}, when we take the partial trace the remaining cross terms  will be of our interest. When we discuss the property of ultra-orthogonality, we stated that the cross terms of $\operatorname{Tr}_{E}\ket{E_n}\bra{E_m}$ should be the ones that had to be near to zero, in order for thermalisation to occur on the specific case we discussed in chapter $1$. Here we are going to formalise all these ideas.\\

\indent We can start by choosing an arbitrary state such that it can be decomposed in its vectors of the Fock basis as\footnote{Note that we can use the Fock basis since it is naturally the basis in which we diagonalise our Hamiltonian, therefore it is an energy basis as well.}
\begin{equation}
\ket{\psi} = \sum_{\vec{n}}\psi(\vec{n})\ket{\vec{n}}.
\end{equation}
Therefore we are interested in studying the terms of the form\footnote{Here we denote the partial trace by $\operatorname{Tr}_{n/L}$ meaning that for the corresponding sequence of length $N$ in the Fock Space we take the respective $L$ elements, meaning that $N-L$ elements will be regarded as our environment.}
\begin{equation}
\hat{X}_{ij} \doteq \operatorname{Tr}_{E}\left(\ket{\vec{n}_i}\bra{\vec{n}_j}\right) \equiv \operatorname{Tr}_{N/L}\left(\ket{\vec{n}_i}\bra{\vec{n}_j}\right).
\end{equation}
\indent What ultra-orthogonality tells us is that this term has to be zero or in the worst scenario something of the order of fluctuations. Since we are working on the very special case of Quadratic Hamiltonian describing Fermionic systems, we recall the fact that the operator is generated by the Majorana operators, and form the so called Clifford algebra, described in section 2.2. We also saw that these operators can be mapped to the Grassman variables, which allow us to compute things like observables. Taking into account that we started with a space in which we had to deal with $N$ Pauli operator and we changed to a new space in which we work with $2N$ spinless operators and the fact that the most general  function we can built over the Grassman algebra is polynomial of the Grassmann variables. Thus we construct a function of the Grassman variables which takes two binary sequences ($\vec{x},\vec{y}$), $x_i,y_i \in \{0,1\}$, lets call it $\gamma(\vec{x},\vec{y})$. The reason for defining this function, is that we are going to describe the system in the space of size $L$ with its correspondent operators, meaning that these sequences can not be arbitrary, they must have to be sequences such that after its first $L$ elements, they must have only zeros. To illustrate this, consider the following example, let $\vec{x}=(0100\ldots 0)$, $\vec{y}=(1100\ldots 0)$, here $L=3$, so our function will be described by\footnote{Here we have changed the original notation given in \eqref{CH2:majorana} and change it by 
\begin{equation}
\hat{c}_{2j-1}\rightarrow \gamma_{j} , \qquad \hat{c}_{2j}\rightarrow \gamma_{N+j},
\end{equation}
for the case of $N$ operators.
}
\begin{equation}
\gamma(\vec{x},\vec{y}) = \gamma_1^{x_1=0} \gamma_{4}^{y_1=1} \gamma_{2}^{x_2=1} \gamma_{5}^{y_2=1} \gamma_{3}^{x_3=0} \gamma_{6}^{y_3=1} = \gamma_{4}\gamma_{2}\gamma_{5}\gamma_{6},
\end{equation}
Note that in this way we are able to write any product of Grassmann operators. in general, for two sequences, and a fixed size $L$, this function will be given by
\begin{equation}
\gamma(\vec{x},\vec{y}) = \gamma_1^{x_1} \gamma_{L+1}^{y_1} \gamma_{2}^{x_2} \gamma_{L+2}^{y_2}\ldots \gamma_{L}^{x_L} \gamma_{2L}^{y_L},
\end{equation} 
where this function has the property that
\begin{equation}
\gamma(\vec{x},\vec{y})\gamma(\vec{x}',\vec{y}') = e^{i\phi(\vec{x},\vec{y},\vec{x}',\vec{y}')} \gamma(\vec{x}+\vec{x}',\vec{y}+\vec{y}').
\label{CH2:my_relation_delta}
\end{equation}
The phase appears as a consequence of the anti-commutation relation and $\phi(\vec{x},\vec{y},\vec{x}',\vec{y}')$ is a function that would depend on the weight of the sequences $\vec{x},\vec{y},\vec{x}'$and $\vec{y}'$.\\

\indent This provides a set of operators that live in the space of size $L$, and that will allow us to expand our operator $\hat{X}_{ij}$,\footnote{For illustrate these ideas we first suppose we have the operators in order. However, we will generalise it later.}
\begin{equation}
\hat{X}_{ij} = \sum_{\vec{x},\vec{y}} f(\vec{x},\vec{y})\gamma(\vec{x},\vec{y}).
\end{equation}
Our task will then be to find the coefficient $f(\vec{x},\vec{y})$, this can be achieved by multiplying on both sides by $\gamma(\vec{x'},\vec{y'})$ and taking the trace over $L$
\begin{equation}
\operatorname{Tr}_L\left(\hat{X}_{ij}\gamma^{\dagger}(\vec{x}',\vec{y}')\right)=\sum_{\vec{x},\vec{y}}f(\vec{x},\vec{y}) \operatorname{Tr}_L\left(\gamma(\vec{x},\vec{y})\gamma^{\dagger}(\vec{x}',\vec{y}')\right).
\end{equation}
The right hand side of this equation can be combined with equation \eqref{CH2:my_relation_delta} and deduce that it will give us a delta ($\delta_{\vec{x}+\vec{x}'}\delta_{\vec{y}+\vec{y}'}$). Thus the coefficients $f(\vec{x},\vec{y})$ are given by
\begin{equation}
f(\vec{x}',\vec{y}') = \frac{1}{2^L}\operatorname{Tr_L}\left(\hat{X}_{ij}\gamma^{\dagger}(\vec{x}',\vec{y}')\right)=\frac{1}{2^L} \bra{\vec{n}_j}\gamma^{\dagger}(\vec{x}',\vec{y}')\ket{\vec{n}_i}.
\label{CH2:my_relation_coefficients}
\end{equation}
\indent Thus we first have to know how the operators $\gamma$ acts over the states $\ket{\vec{n}_i}$. For this we recall the fact that the Hamiltonians are diagonalised via an orthogonal transformation that links what we call the spacial modes and the normal modes
\begin{equation}
\overbrace{\gamma_{i_1}\gamma_{i_2}\ldots\gamma_{i_k}}^{\text{Spacial modes}}= O_{i_1\alpha_1}O_{i_2\alpha_2}\ldots O_{i_k\alpha_k} \underbrace{\gamma_{\alpha_1}\gamma_{\alpha_2}\ldots\gamma_{\alpha_k}}_{\text{Normal modes}}.
\end{equation}
\indent Note that this operators are diagonal over our $\vec{x}$'s and $\vec{y}$'s, which will allow us to operate over the states $\ket{\vec{n}_i}$.\footnote{By using the function $\gamma(\vec{x},\vec{y})$ there are two ways of getting an specific state $\ket{\vec{n}_i}$
\begin{equation}
\ket{n_i}=\gamma(\vec{n}_i,0)\ket{0}, \qquad \ket{n_i}=\gamma(0,\vec{n}_i)\ket{0}e^{i\phi(\vec{n_i})},
\end{equation}
therefore we can say that the $\vec{x}$'s takes the $0$ and turn them into a $1$, and the $\vec{y}$'s take $0$ and transform it into a one multiplied by a phase.
}. The equation \eqref{CH2:my_relation_coefficients} simplifies to
\begin{equation}
\bra{\vec{n}_j}\gamma(\vec{x},\vec{y})\ket{\vec{n}_i}=\delta_{\vec{n}_i+\vec{x}+\vec{y},\vec{n}_j}e^{i\phi(\vec{n}_i,\vec{n}_j,\vec{x},\vec{y})},
\label{CH2:my_relation_coeficients_2}
\end{equation}
whereas the coefficients $f(\vec{x},\vec{y})$ will be given by
\begin{equation}
f(\vec{x},\vec{y}) = \frac{1}{2^L}\sum_{\vec{x}',\vec{y}'} \mathcal{U}_{\vec{x}\vec{x}'} \mathcal{V}_{\vec{y}\vec{y}'} \underbrace{\bra{\vec{n}_j}\gamma(\vec{x},\vec{y})\ket{\vec{n}_i}}_{\propto \delta_{\vec{n}_i+\vec{n}_j},\vec{x}+\vec{y}}.
\label{CH2:my_Relation_coeffitients_final}
\end{equation}
\indent In equation \eqref{CH2:my_relation_coeficients_2} the delta can be changed for $\delta_{\vec{n}_i+\vec{n_j},\vec{x}+\vec{y}}$, and the reason to do so is because we are working with arithmetic modulo $2$ and we can see that the term $\vec{n}_i+\vec{n}_j$ is nothing but the vector of differences, which means that the only values different than zero in this vector are when $n_{i_k}\neq n_{j_k}$. This result is extremely important because it tell us that whenever we work with states like $\hat{X}_{ij}$, if the vector of differences have more ones than the vector of differences $\vec{x}+\vec{y}$. Note that the vector of differences given by $\vec{x}+\vec{y}$ can have at most $L$ errors, which means that whenever the number of ones in the difference vector defined by the states $\vec{n}_i+\vec{n}_j$ exceeds $L$, the state $\hat{X}_{ij}$ will be immediately zero. This turns out to be an astonishing result and bring even more questions, like, how likely is it to have more than $L$ errors when $N\gg L$?, what happen when we have less errors is this quantity still small enough as we expected?. These questions will be tackled in a moment but something to stress is the fact that it is the branch point of our study. We will be first addressing the first question and afterwards we will talk about the second one.
\section{Fermionic Random Minimum Codes.}
Let us recapitulate a little more what we have done in the latter section. We define two binary sequences of excitations $\vec{n}_i$, $\vec{n}_j$, and the vector of differences $\vec{e}_{ij}=\vec{n}_i$+$\vec{n}_j$, we will denote the distance between these two binary sequences by
\begin{equation}
\begin{aligned}
d&=W(\vec{e}_{ij})\rightarrow \text{Weight of }\vec{e}_{ij} \\
&=d_{H}(\vec{n}_i, \vec{n}_j) \rightarrow \text{Hamming distance.}
\end{aligned}
\end{equation}
We show that there is a specific distance $d>L$ at which the state $\hat{X}_{ij}=\operatorname{Tr}_{N/L}\left(\bra{\vec{n}_i}\ket{\vec{n}_j}\right)$ is equal to zero $\hat{X}_{ij}$. This might sound quite familiar since it is connected to minimum distance codes and to see this more clearly, consider a set of binary codewords
\begin{equation}
\mathcal{C}=\{\vec{x}^{(1)},\vec{x}^{(2)},\ldots,\vec{x}^{(2^k)}\} \quad \vec{x}\in\{0,1\}^{N},
\end{equation}
and size $|\mathcal{C}|=2^{NR}$, with $R=k/N$ as defined in \eqref{CH2:Rate_of_code}. Analogously, consider the Hilbert space
\begin{equation}
\mathcal{H}_{\mathcal{C}}=\operatorname{Span}\left(\ket{\vec{x}^{(1)}},\ket{\vec{x}^{(2)}},\ldots,\ket{\vec{x}^{(2^k)}}\right).
\end{equation}
It is easy to check that $|\mathcal{H}_{\mathcal{C}}|=|\mathcal{C}|=2^{NR}$. From the previous results, we can ensure that there exists a Hilbert space with dimension $\operatorname{dim}|\mathcal{H}_{\mathcal{C}}|=2^{N(1-\mathcal{H}(\ell))}$, where $\ell = L/N$, the relative distance. More specifically, $\forall \ket{psi}\in \mathcal{H}_{\mathcal{C}}$,
\begin{equation}
\ket{\psi} = \sum_{\vec{n}\in\mathcal{C}}\psi(\vec{n})\ket{\vec{n}},
\end{equation}
where the code $\mathcal{C}$ refers to a code $(N,k,\ell)$, $d>\ell$. So 
\begin{equation}
\rho_{L}(\psi) = \operatorname{Tr}_{N/L}\left(\ket{\psi}\bra{\psi}\right) = \sum |\psi(\vec{n})|^2 \ket{\vec{n}}\bra{\vec{n}}.
\end{equation}
\indent Thus, the states belonging to $ \mathcal{H}_{\mathcal{C}}$ are like frozen states, meaning that
\begin{equation}
\frac{d\rho_L}{dt}=0.
\end{equation}
\indent One of the most important question one could ask is, what is the biggest code with relative distance $\ell$ constrained with certain value of energy?. Namely, denoting by
\begin{equation}
S(\ell) = \bigcup_{\mathcal{C}, \delta>\ell}\mathcal{H}_{\mathcal{C}},
\end{equation} 
the set of codes of minimum distance $\ell$. So we would like to know how big this set is and estimate its size, and for doing that, its correspondent error exponent will be needed. To do so, we first define our problem in terms of random variables. Let $X_i(\theta_k)$ be a random probability that takes the value $1$ with probability $p(\theta_k)$ and the value $0$ with probability $1-p(\theta_k)$, and let $X=\sum_{i}X_i(\theta_k)$ be the sum of these random variables, or equivalently the number of errors. Then we ask ourselves about the probability of having certain number of errors in our fermionic code. Particularly, we ask for the probability of having a number of errors greater or equal to a certain quantity. To address this question we can make use of the Chernoff inequality\footnote{In this part we do not specify what distribution is the one we are choosing, one might guess that it is related to the Fermi-Dirac distribution, but our result will be in terms of the this general distribution $p(\theta_k)$. In the next chapter we will provide an expression for the case of the $XY$ model.} 
\begin{equation}
\begin{aligned}
P(e^{S X}\geq e^{S d})&\leq \min_{S}\left\langle e^{S X}\right\rangle e^{-S d}\\
P(e^{-S X}\geq e^{-S d})&\leq \min_{S}\left\langle e^{-S X}\right\rangle e^{S d}.
\end{aligned}
\end{equation}
\indent We then compute the expected value $\left\langle e^{S X}\right\rangle$ taking into account that we are working with independent variables
\begin{equation}
\left\langle e^{S X}\right\rangle = \prod_{k}  E(e^{S X(\theta_k})) = \prod_k \left(1+p(\theta_k)(e^S -1)\right) = e^{\sum_{k}\log(1+p(\theta_k)(e^S -1))}\equiv e^{-Nr(\delta)},
\end{equation}
where $r(\delta)$ corresponds to the correspondent error exponent
\begin{equation}
r(\delta) = \min_{S} \frac{1}{N} \left(\log\langle e^{SX}\rangle - S\delta\right).
\end{equation}
\indent Since we are interested in the case when $N\to \infty$, the error exponent can be written as
\begin{equation}
r(\delta)\stackrel{N\to \infty}{\equiv}\min_{S}\oint \frac{d\theta}{2\pi}\log\left(1+p(\theta)(e^S -1)\right) -S\delta,
\label{CH2:Error_exponent}
\end{equation}
which, even though can not be analytically solved, it can be numerically solved by differentiating with respect to $S$, to obtain
\begin{equation}
\delta = \oint \frac{d\theta}{2\pi} \frac{p(\theta)e^{S}}{1-p(\theta) + p(\theta)e^S}.
\end{equation}
Thus we conclude that the mean number of codes at a distance $d$ is given by
\begin{equation}
\langle S_{\mathcal{C}}(d)\rangle = {M \choose 2} 2^{-Nr(\delta)},
\end{equation}
with $M\equiv 2^{NR}$, we conclude that when ever we work with rates lower than $r(\delta)/2$ the average number of pairs at a distance $d$ goes to zero exponentially. Otherwise the number of pairs that have minimum distance $d$ isexponentially large
\begin{equation}
\langle S_{\mathcal{C}}(d)\rangle \doteq  2^{N(2R-r(\delta))}.
\end{equation}
With the latter equation we answer one of our main questions, showing that indeed there exist exponentially large Hilbert subspaces such that its reduced states are automatically constant. However, we have not seen what happens in the case when we work with distances less than $L$. In the next section we will discuss a little bit more about it, and we will provide the intuition behind this case in order to have the full picture of how ultra-orthogonality may work in fermionic systems.
\subsection{Case when the numbers of errors is less than $L$}
For this case we know that the term $\hat{X}_{ij}$ is generally different from zero, however, we are going to show that while it is not necessarily zero, this quantity will be intuitively small. To study this we will consider the norm of $||\hat{X}_{ij}||$. If we take into account the relation found in \eqref{CH2:my_Relation_coeffitients_final}, we will find that
\begin{equation}
\begin{aligned}
||\hat{X}_{ij}||^{2}&=\operatorname{Tr}\left(\hat{X}_{ij} \hat{X}^{\dagger}_{ij}\right) = \frac{1}{2^L}\sum_{\vec{x},\vec{y}}|f(\vec{x},\vec{y})|^2\\
&=\frac{1}{2^L}\underset{\vec{x}'',\vec{y}''}{\sum_{\vec{x},\vec{y}}}\left(	\mathcal{U}_{\vec{x}\vec{x}'} \mathcal{U}_{\vec{x}\vec{x}''}\right) \left(	\mathcal{V}_{\vec{y}\vec{y}'} \mathcal{V}_{\vec{y}\vec{y}''}\right) e^{\phi(\vec{x},\vec{y}')\phi(\vec{x}'',\vec{y}'')}\delta_{\vec{n}_i+\vec{n}_j,\vec{x}'+\vec{y}'}\delta_{\vec{n}_i+\vec{n}_j,\vec{x}''+\vec{y}''}.
\end{aligned}
\label{CH2:Norm_x_i_j_operator}
\end{equation}
\indent Our purpose will then be to bound the terms $\sum_{\vec{x}}\left(	\mathcal{U}_{\vec{x}\vec{x}'} \mathcal{U}_{\vec{x}\vec{x}''}\right)$. The reason for this is because if we can bound these terms by some quantity, this bound will also holds for the part containing the $\vec{y}$'s, thus for the rest of this work we will working with this quantity instead of working with \eqref{CH2:Norm_x_i_j_operator}. First, notice that the expression in \eqref{CH2:my_Relation_coeffitients_final} will not be too useful to us, since we are more interested in changing basis, for example we will be constantly changing from the spacial modes to the normal modes, so, by writing in this way the products of the operators we are not taking into account the geometric meaning of these terms. In the expansion of an operator, the resulting terms from $\gamma(\vec{x},\vec{y})$ have to be interpreted as $p-$forms, meaning that this will correspond to a volume generated by some vectors. So if we want to take into account the fact that volumes change over basis, we should have to understand that the products of the operators in $\gamma(\vec{x},\vec{y})$ should transform in a particular way to take this into account. It is not trivial, but is simple to check that the way this products should change in order to transform as Grassmann variables and take into account the change on the volumes over transformation is via the antisymmetrizing operator. So when we see the products such as $\gamma_{1}\gamma_{2}\gamma_{3}$ we have to understand it as $\frac{1}{3!}\gamma_{[1}\gamma_2\gamma_{3]}$. Generalising these ideas we consider a general $p-$form, lets say $\gamma(\vec{\alpha})$
\begin{equation}
\gamma(\vec{\alpha}) =_{[}\gamma_{\alpha_1}\gamma_{\alpha_2}\gamma_{\alpha_2}\ldots\gamma_{\alpha_p]},
\end{equation}
if each of these elements transform as $\gamma_{\alpha_i} = O_{\alpha_i j} \tilde{\gamma}_{j}$. The $p-$ form will transform as
\begin{equation}
\gamma(\vec{\alpha}) = \operatorname{det}\left(O\big|_{\vec{\alpha},\vec{\beta}}\right)\gamma_{\vec{\beta}},
\label{CH2:transformations_of_operators}
\end{equation}
where the term $\operatorname{det}\left(O\big|_{\vec{\alpha},\vec{\beta}}\right)$ refers to the minor of the matrix $O$. Turning back to our main problem look that the term of $\sum_{\vec{x}}\left(	\mathcal{U}_{\vec{x}\vec{x}'} \mathcal{U}_{\vec{x}\vec{x}''}\right)$ can be written in terms of this determinants as
\begin{equation}
\sum_{\vec{x}}\left(	\mathcal{U}_{\vec{x}\vec{x}'} \mathcal{U}_{\vec{x}\vec{x}''}\right) = \operatorname{det}\left[\left(O\Pi O^T\right)\big|_{\vec{x}',\vec{x}''}\right].
\label{CH2:Equation_to_find_bound}
\end{equation}
\indent Therefore if we are able to find a bound for this determinant we could show that the quantity $||\hat{X}_{ij}||$ is indeed small as we have been saying. Nonetheless, to bound this quantity in general is not an easy task and we can not show a general bound for this quantity, from the previous arguments based on typicality we expect $||\hat{X}_{ij}||$ to be bounded by a small quantity as we expected from the arguments aforementioned. Of course, one could explore this quantity for specific systems and find bounds for those particular cases and find the physical meaning associated with it\footnote{Specifically we have computed numerically the values of  \eqref{CH2:Equation_to_find_bound} for the case of the one dimensional $XY$ model and we have found that these values are for most cases extremely small.}, however, it is not on the scope of this work to explore this for particular cases and we leave it a a future work.


% %it is  As we will show the quantity $||\hat{X}_{ij}||$ for the case of this model is indeed small, and then it provide us an intuition that there must be some mechanism over the other models such that this quantity remains small for other Fermionic systems.\\

%%\indent In the next chapter we are going to present the one dimensional $XY$ model, its importance for our study as well as some calculations of the previous results to illustrate the behaviour of ultra-orthogonality for this specific case.
%and two arbitrary sequences $\vec{x}$, $\vec{y}$



%as the first one we will talk about why not as general as the first one we will discuss why  be bounded by a small quantity and therefore  part and in the second part we will show how we can also study the   
%of  Thus let's first consider an state $\ket{\psi}$ which has an expansion in terms of vectors in the Fock space, this is



%When we work with this ensemble it is easy to note  the probability distribution over any such set in the same as that in the RCE. 

%if $\{\mathbf{x}_k\}$ is the set of $K'\leq K$ linearly independet information $K-$tuples, then the $K'$ corresponding codewords $\{\mathbf{x}(\mathbf{u}_k)\}$ are equally likely to be any of the $2^{NK'}$ possible sets of $K'$ binary $N-$tuples. 


% As mentioned in \cite{barg_random_2002}, typical linear codes (TLC) achieve the best lower bound on error exponent at all rates, and similarly for the case of the typical random codes (TRC), which lies between the random coding exponent $E_r(R)$   \cite{domb_random_2016}


